# Text-to-Image Generation using GANs ğŸ¦ğŸ§ ğŸ¨

This project demonstrates **Text-to-Image Generation** using **Generative Adversarial Networks (GANs)** trained on the **CUB-200-2011 Birds Dataset**. Developed as part of the Alternate Assessment Tool (AAT) for the subject **Machine Learning (22EC554)** under the Department of Electronics & Communication Engineering, DSCE (2024-25).

## âœ¨ Abstract

Using models like **StackGAN** and **AttnGAN**, this system converts textual bird descriptions into realistic synthetic images. The system helps overcome the challenge of limited labeled data, with practical applications in **data augmentation**, **ecological modeling**, and **AI training**.

## ğŸ§  Algorithms & Concepts

- Generative Adversarial Networks (GANs)
- Text Embedding using Word2Vec/BERT
- KL Divergence Loss
- Deep Convolutional Neural Networks (DCNN)
- Attention Mechanisms
- Perceptual Loss

## ğŸ“Š Results

- Epoch 1: Discriminator Loss = 1.32, Generator Loss = 1.65
- Epoch 10: Discriminator Loss = 1.00, Generator Loss = 1.14
- Epoch 20: Discriminator Loss = 0.88, Generator Loss = 1.29

Below are example images generated by the model over multiple training epochs:

### ğŸ£ Epoch 1
![Epoch 1 Output](outputs/epoch_1.png)

### ğŸ¦ Epoch 10
![Epoch 10 Output](outputs/epoch_10.png)

### ğŸ¯ Final Output (Epoch 20+)
![Final Output](outputs/final_output.png)

- **Epoch 1** â€“ Basic bird-like shapes and color blobs appear
- **Epoch 10** â€“ Form and color start aligning with textual embeddings
- **Final** â€“ Generated images resemble the described birds more accurately

  
## ğŸš€ Future Enhancements

- Real-time prompt input
- High-resolution image generation
- Faster inference
- Interactive image customization


## ğŸ“œ License

MIT License
