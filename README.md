# Text-to-Image Generation using GANs ğŸ¦ğŸ§ ğŸ¨

This project demonstrates **Text-to-Image Generation** using **Generative Adversarial Networks (GANs)** trained on the **CUB-200-2011 Birds Dataset**. Developed as part of the Alternate Assessment Tool (AAT) for the subject **Machine Learning (22EC554)** under the Department of Electronics & Communication Engineering, DSCE (2024-25).

## âœ¨ Abstract

Using models like **StackGAN** and **AttnGAN**, this system converts textual bird descriptions into realistic synthetic images. The system helps overcome the challenge of limited labeled data, with practical applications in **data augmentation**, **ecological modeling**, and **AI training**.

## ğŸ§  Algorithms & Concepts

- Generative Adversarial Networks (GANs)
- Text Embedding using Word2Vec/BERT
- KL Divergence Loss
- Deep Convolutional Neural Networks (DCNN)
- Attention Mechanisms
- Perceptual Loss

## ğŸ“Š Results

- Epoch 1: Discriminator Loss = 1.32, Generator Loss = 1.65
- Epoch 10: Discriminator Loss = 1.00, Generator Loss = 1.14
- Epoch 20: Discriminator Loss = 0.88, Generator Loss = 1.29

## ğŸš€ Future Enhancements

- Real-time prompt input
- High-resolution image generation
- Faster inference
- Interactive image customization


## ğŸ“œ License

MIT License
